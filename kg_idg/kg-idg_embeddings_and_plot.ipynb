{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "122bf290",
   "metadata": {},
   "source": [
    "# Embeddings and Plot for KG-IDG\n",
    "(incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dec84b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "from ensmallen import Graph\n",
    "from embiggen.pipelines import compute_node_embedding\n",
    "from plot_keras_history import plot_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15c6d998",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/merged/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12231/3193177766.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/merged/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0medges_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"merged-kg_edges.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnodes_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"merged-kg_nodes.tsv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/merged/'"
     ]
    }
   ],
   "source": [
    "# Retrieve graph\n",
    "! wget https://kg-hub.berkeleybop.io/kg-idg/20211112/KG-IDG.tar.gz\n",
    "! tar -xvzf KG-IDF.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3710c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_path = \"merged-kg_edges.tsv\"\n",
    "nodes_path = \"merged-kg_nodes.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "890ba241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>kg-idg</h2><p>The undirected graph kg-idg has 944952 nodes and 3287390 edges.</p><h3>RAM requirements</h3><p>The RAM requirements for the nodes and edges data structures are 262.32MB and 15.85MB respectively.</p><h3>Degree centrality</h3><p>The minimum node degree is 0, the maximum node degree is 85052, the mode degree is 1, the mean degree is 6.96 and the node degree median is 1.</p><p>The nodes with highest degree centrality are: <a href='http://purl.obolibrary.org/obo/chebi#2_STAR' target='_blank' title='Go to Open Biomedical Ontology to get more informations about OBO:chebi#2_STAR'>OBO:chebi#2_STAR</a> (degree 85052), <a href='http://purl.obolibrary.org/obo/chebi#3_STAR' target='_blank' title='Go to Open Biomedical Ontology to get more informations about OBO:chebi#3_STAR'>OBO:chebi#3_STAR</a> (degree 59387), <a href='http://purl.obolibrary.org/obo/IAO_0000227' target='_blank' title='Go to Information Artifact Ontology to get more informations about IAO:0000227'>IAO:0000227</a> (degree 22057), <a href='http://www.sequenceontology.org/browser/current_svn/term/SO:0000704' target='_blank' title='Go to Sequence Ontology to get more informations about SO:0000704'>SO:0000704</a> (degree 16459) and <a href='https://www.ebi.ac.uk/chebi/searchId.do?chebiId=CHEBI:24995' target='_blank' title='Go to CHEBI to get more informations about CHEBI:24995'>CHEBI:24995</a> (degree 7341).</p><h3>Disconnected nodes</h3><p>Disconnected nodes are nodes that are not connected to any other node.The graph contains 55251 disconnected nodes.</p><h4>Singleton nodes</h4><p>Singleton nodes are nodes with no edge to other nodes nor selfloops. The graph contains 55210 singleton nodes, which are <a href='https://www.ensembl.org/Gene/Summary?g=ENSG00000004059' target='_blank' title='Go to ENSEMBL to get more informations about ENSEMBL:ENSG00000004059'>ENSEMBL:ENSG00000004059</a>, <a href='https://www.ensembl.org/Gene/Summary?g=ENSG00000168280' target='_blank' title='Go to ENSEMBL to get more informations about ENSEMBL:ENSG00000168280'>ENSEMBL:ENSG00000168280</a>, <a href='https://www.ensembl.org/Gene/Summary?g=ENSG00000047936' target='_blank' title='Go to ENSEMBL to get more informations about ENSEMBL:ENSG00000047936'>ENSEMBL:ENSG00000047936</a>, <a href='https://www.ensembl.org/Gene/Summary?g=ENSG00000145780' target='_blank' title='Go to ENSEMBL to get more informations about ENSEMBL:ENSG00000145780'>ENSEMBL:ENSG00000145780</a> and <a href='https://www.ensembl.org/Gene/Summary?g=ENSG00000149923' target='_blank' title='Go to ENSEMBL to get more informations about ENSEMBL:ENSG00000149923'>ENSEMBL:ENSG00000149923</a>, plus other 55205 singleton nodes.</p><h4>Singleton nodes with selfloops</h4><p>Singleton nodes with selfloops are nodes with no edge to other nodes and have exclusively selfloops. The graph contains 41 singleton nodes with selfloops, which are :MGI:MGI:1924846 (degree 1), :OBO:go&#x2f;extensions&#x2f;reacto.owl#REACTO_R-HSA-211223 (degree 1), <a href='http://zfin.org/ZDB-GENE-020812-1' target='_blank' title='Go to ZFIN to get more informations about ZFIN:ZDB-GENE-020812-1'>ZFIN:ZDB-GENE-020812-1</a> (degree 1), :OBO:go&#x2f;extensions&#x2f;reacto.owl#REACTO_R-HSA-904821 (degree 1) and :OBO:go&#x2f;extensions&#x2f;reacto.owl#REACTO_R-HSA-8851105 (degree 1), plus other 36 singleton nodes with selfloops.</p>"
      ],
      "text/plain": [
       "<h2>kg-idg</h2><p>The undirected graph kg-idg has 944952 nodes and 3287390 edges.</p><h3>RAM requirements</h3><p>The RAM requirements for the nodes and edges data structures are 262.32MB and 15.85MB respectively.</p><h3>Degree centrality</h3><p>The minimum node degree is 0, the maximum node degree is 85052, the mode degree is 1, the mean degree is 6.96 and the node degree median is 1.</p><p>The nodes with highest degree centrality are: <a href='http://purl.obolibrary.org/obo/chebi#2_STAR' target='_blank' title='Go to Open Biomedical Ontology to get more informations about OBO:chebi#2_STAR'>OBO:chebi#2_STAR</a> (degree 85052), <a href='http://purl.obolibrary.org/obo/chebi#3_STAR' target='_blank' title='Go to Open Biomedical Ontology to get more informations about OBO:chebi#3_STAR'>OBO:chebi#3_STAR</a> (degree 59387), <a href='http://purl.obolibrary.org/obo/IAO_0000227' target='_blank' title='Go to Information Artifact Ontology to get more informations about IAO:0000227'>IAO:0000227</a> (degree 22057), <a href='http://www.sequenceontology.org/browser/current_svn/term/SO:0000704' target='_blank' title='Go to Sequence Ontology to get more informations about SO:0000704'>SO:0000704</a> (degree 16459) and <a href='https://www.ebi.ac.uk/chebi/searchId.do?chebiId=CHEBI:24995' target='_blank' title='Go to CHEBI to get more informations about CHEBI:24995'>CHEBI:24995</a> (degree 7341).</p><h3>Disconnected nodes</h3><p>Disconnected nodes are nodes that are not connected to any other node.The graph contains 55251 disconnected nodes.</p><h4>Singleton nodes</h4><p>Singleton nodes are nodes with no edge to other nodes nor selfloops. The graph contains 55210 singleton nodes, which are <a href='https://www.ensembl.org/Gene/Summary?g=ENSG00000004059' target='_blank' title='Go to ENSEMBL to get more informations about ENSEMBL:ENSG00000004059'>ENSEMBL:ENSG00000004059</a>, <a href='https://www.ensembl.org/Gene/Summary?g=ENSG00000168280' target='_blank' title='Go to ENSEMBL to get more informations about ENSEMBL:ENSG00000168280'>ENSEMBL:ENSG00000168280</a>, <a href='https://www.ensembl.org/Gene/Summary?g=ENSG00000047936' target='_blank' title='Go to ENSEMBL to get more informations about ENSEMBL:ENSG00000047936'>ENSEMBL:ENSG00000047936</a>, <a href='https://www.ensembl.org/Gene/Summary?g=ENSG00000145780' target='_blank' title='Go to ENSEMBL to get more informations about ENSEMBL:ENSG00000145780'>ENSEMBL:ENSG00000145780</a> and <a href='https://www.ensembl.org/Gene/Summary?g=ENSG00000149923' target='_blank' title='Go to ENSEMBL to get more informations about ENSEMBL:ENSG00000149923'>ENSEMBL:ENSG00000149923</a>, plus other 55205 singleton nodes.</p><h4>Singleton nodes with selfloops</h4><p>Singleton nodes with selfloops are nodes with no edge to other nodes and have exclusively selfloops. The graph contains 41 singleton nodes with selfloops, which are :MGI:MGI:1924846 (degree 1), :OBO:go&#x2f;extensions&#x2f;reacto.owl#REACTO_R-HSA-211223 (degree 1), <a href='http://zfin.org/ZDB-GENE-020812-1' target='_blank' title='Go to ZFIN to get more informations about ZFIN:ZDB-GENE-020812-1'>ZFIN:ZDB-GENE-020812-1</a> (degree 1), :OBO:go&#x2f;extensions&#x2f;reacto.owl#REACTO_R-HSA-904821 (degree 1) and :OBO:go&#x2f;extensions&#x2f;reacto.owl#REACTO_R-HSA-8851105 (degree 1), plus other 36 singleton nodes with selfloops.</p>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = Graph.from_csv(name=\"kg-idg\",\n",
    "                    edge_path=edges_path,\n",
    "                    sources_column=\"subject\",\n",
    "                    destinations_column=\"object\",\n",
    "                    edge_list_header = True,\n",
    "                    edge_list_separator=\"\\t\",\n",
    "                    node_path = nodes_path,\n",
    "                    nodes_column = \"id\",\n",
    "                    node_list_header = True,\n",
    "                    node_list_separator=\"\\t\",\n",
    "                    directed=False,\n",
    "                    verbose=True\n",
    "                    )\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea1496cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 13:42:35.443732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 13:42:35.545812: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 13:42:35.546412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 13:42:37.879876: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-16 13:42:37.881845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 13:42:37.882743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 13:42:37.883299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 13:42:39.117719: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 13:42:39.118417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 13:42:39.118581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2021-11-16 13:42:39.119045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2021-11-16 13:42:39.119363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2109 MB memory:  -> device: 0, name: Quadro T2000, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a27d57d5d2a14519ba8da85dc3e5597a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 13:42:39.872793: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:766] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_237\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020FlatMapDataset:1\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_FLOAT\n",
      "      type: DT_FLOAT\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Error reported to Coordinator: OOM when allocating tensor with shape[944952,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 346, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 696, in wrapper\n",
      "    return converted_call(f, args, kwargs, options=options)\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 383, in converted_call\n",
      "    return _call_unconverted(f, args, kwargs, options)\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 464, in _call_unconverted\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step\n",
      "    outputs = model.train_step(data)\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/engine/training.py\", line 816, in train_step\n",
      "    self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 532, in minimize\n",
      "    return self.apply_gradients(grads_and_vars, name=name)\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 639, in apply_gradients\n",
      "    self._create_all_weights(var_list)\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 830, in _create_all_weights\n",
      "    self._create_slots(var_list)\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/nadam.py\", line 98, in _create_slots\n",
      "    self.add_slot(var, 'v')\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 916, in add_slot\n",
      "    weight = tf.Variable(\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/initializers/initializers_v2.py\", line 144, in __call__\n",
      "    return tf.zeros(shape, dtype)\n",
      "tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[944952,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 13:42:50.324465: W tensorflow/core/common_runtime/bfc_allocator.cc:462] Allocator (GPU_0_bfc) ran out of memory trying to allocate 360.47MiB (rounded to 377980928)requested by op Fill\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2021-11-16 13:42:50.324553: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] BFCAllocator dump for GPU_0_bfc\n",
      "2021-11-16 13:42:50.324566: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (256): \tTotal Chunks: 18, Chunks in use: 18. 4.5KiB allocated for chunks. 4.5KiB in use in bin. 92B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324570: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (512): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324573: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324576: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324578: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324581: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324583: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324585: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324588: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324590: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324593: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324595: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324597: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324600: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (2097152): \tTotal Chunks: 2, Chunks in use: 2. 7.21MiB allocated for chunks. 7.21MiB in use in bin. 7.21MiB client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324603: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324606: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (8388608): \tTotal Chunks: 1, Chunks in use: 0. 13.27MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324608: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324611: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324613: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324616: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324619: I tensorflow/core/common_runtime/bfc_allocator.cc:1017] Bin (268435456): \tTotal Chunks: 6, Chunks in use: 4. 2.04GiB allocated for chunks. 1.41GiB in use in bin. 1.41GiB client-requested in use in bin.\n",
      "2021-11-16 13:42:50.324624: I tensorflow/core/common_runtime/bfc_allocator.cc:1033] Bin for 360.47MiB was 256.00MiB, Chunk State: \n",
      "2021-11-16 13:42:50.324630: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 286.56MiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 256B | Requested Size: 8B | in_use: 1 | bin_num: -1\n",
      "2021-11-16 13:42:50.324635: I tensorflow/core/common_runtime/bfc_allocator.cc:1039]   Size: 360.47MiB | Requested Size: 0B | in_use: 0 | bin_num: 20, prev:   Size: 360.47MiB | Requested Size: 360.47MiB | in_use: 1 | bin_num: -1, next:   Size: 360.47MiB | Requested Size: 360.47MiB | in_use: 1 | bin_num: -1\n",
      "2021-11-16 13:42:50.324638: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] Next region of size 2211866112\n",
      "2021-11-16 13:42:50.324643: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 70d160000 of size 1280 next 1\n",
      "2021-11-16 13:42:50.324645: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 70d160500 of size 256 next 2\n",
      "2021-11-16 13:42:50.324647: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 70d160600 of size 256 next 3\n",
      "2021-11-16 13:42:50.324649: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 70d160700 of size 256 next 4\n",
      "2021-11-16 13:42:50.324651: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 70d160800 of size 256 next 5\n",
      "2021-11-16 13:42:50.324654: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 70d160900 of size 377980928 next 26\n",
      "2021-11-16 13:42:50.324656: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 7239d9100 of size 377980672 next 6\n",
      "2021-11-16 13:42:50.324658: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 73a251800 of size 377980928 next 7\n",
      "2021-11-16 13:42:50.324660: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 750aca000 of size 377980928 next 9\n",
      "2021-11-16 13:42:50.324662: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 767342800 of size 377980928 next 10\n",
      "2021-11-16 13:42:50.324665: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77dbbb000 of size 256 next 8\n",
      "2021-11-16 13:42:50.324669: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77dbbb100 of size 3779840 next 11\n",
      "2021-11-16 13:42:50.324673: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df55e00 of size 256 next 12\n",
      "2021-11-16 13:42:50.324676: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df55f00 of size 256 next 13\n",
      "2021-11-16 13:42:50.324680: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df56000 of size 256 next 14\n",
      "2021-11-16 13:42:50.324683: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df56100 of size 256 next 15\n",
      "2021-11-16 13:42:50.324686: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df56200 of size 256 next 16\n",
      "2021-11-16 13:42:50.324689: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df56300 of size 256 next 19\n",
      "2021-11-16 13:42:50.324692: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df56400 of size 256 next 20\n",
      "2021-11-16 13:42:50.324695: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df56500 of size 256 next 21\n",
      "2021-11-16 13:42:50.324699: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df56600 of size 256 next 22\n",
      "2021-11-16 13:42:50.324701: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df56700 of size 256 next 23\n",
      "2021-11-16 13:42:50.324704: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df56800 of size 256 next 24\n",
      "2021-11-16 13:42:50.324708: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df56900 of size 256 next 25\n",
      "2021-11-16 13:42:50.324711: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77df56a00 of size 3779840 next 27\n",
      "2021-11-16 13:42:50.324714: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 77e2f1700 of size 13912832 next 17\n",
      "2021-11-16 13:42:50.324718: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] InUse at 77f036200 of size 256 next 18\n",
      "2021-11-16 13:42:50.324722: I tensorflow/core/common_runtime/bfc_allocator.cc:1066] Free  at 77f036300 of size 300483328 next 18446744073709551615\n",
      "2021-11-16 13:42:50.324725: I tensorflow/core/common_runtime/bfc_allocator.cc:1071]      Summary of in-use Chunks by size: \n",
      "2021-11-16 13:42:50.324731: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 18 Chunks of size 256 totalling 4.5KiB\n",
      "2021-11-16 13:42:50.324736: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2021-11-16 13:42:50.324741: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 2 Chunks of size 3779840 totalling 7.21MiB\n",
      "2021-11-16 13:42:50.324745: I tensorflow/core/common_runtime/bfc_allocator.cc:1074] 4 Chunks of size 377980928 totalling 1.41GiB\n",
      "2021-11-16 13:42:50.324749: I tensorflow/core/common_runtime/bfc_allocator.cc:1078] Sum Total of in-use chunks: 1.42GiB\n",
      "2021-11-16 13:42:50.324753: I tensorflow/core/common_runtime/bfc_allocator.cc:1080] total_region_allocated_bytes_: 2211866112 memory_limit_: 2211866215 available bytes: 103 curr_region_allocation_bytes_: 4423732736\n",
      "2021-11-16 13:42:50.324761: I tensorflow/core/common_runtime/bfc_allocator.cc:1086] Stats: \n",
      "Limit:                      2211866215\n",
      "InUse:                      1519489280\n",
      "MaxInUse:                   1519489280\n",
      "NumAllocs:                          33\n",
      "MaxAllocSize:                377980928\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2021-11-16 13:42:50.324767: W tensorflow/core/common_runtime/bfc_allocator.cc:474] ******************________________*****************************************************_____________\n",
      "2021-11-16 13:42:50.324868: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at constant_op.cc:175 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[944952,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/six.py\", line 719, in reraise\n        raise value\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/engine/training.py\", line 816, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 639, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 830, in _create_all_weights\n        self._create_slots(var_list)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/nadam.py\", line 98, in _create_slots\n        self.add_slot(var, 'v')\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 916, in add_slot\n        weight = tf.Variable(\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/initializers/initializers_v2.py\", line 144, in __call__\n        return tf.zeros(shape, dtype)\n\n    ResourceExhaustedError: OOM when allocating tensor with shape[944952,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_12231/3427501744.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnode_embedding_method_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"CBOW\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m first_order_rw_node_embedding, training_history = compute_node_embedding(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnode_embedding_method_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_embedding_method_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kg-env/lib/python3.8/site-packages/embiggen-0.9.3-py3.8.egg/embiggen/pipelines/compute_node_embedding.py\u001b[0m in \u001b[0;36mcompute_node_embedding\u001b[0;34m(graph, node_embedding_method_name, use_mirrored_strategy, devices, fit_kwargs, verbose, automatically_drop_unsupported_parameters, automatically_enable_time_memory_tradeoffs, automatically_sort_by_decreasing_outbound_node_degree, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;31m# Call the wrapper with cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m     return _compute_node_embedding(\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kg-env/lib/python3.8/site-packages/cache_decorator-2.0.9-py3.8.egg/cache_decorator/cache.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;31m# otherwise compute the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kg-env/lib/python3.8/site-packages/embiggen-0.9.3-py3.8.egg/embiggen/pipelines/compute_node_embedding.py\u001b[0m in \u001b[0;36m_compute_node_embedding\u001b[0;34m(graph, graph_name, node_embedding_method_name, fit_kwargs, verbose, use_mirrored_strategy, devices, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMirroredStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kg-env/lib/python3.8/site-packages/embiggen-0.9.3-py3.8.egg/embiggen/pipelines/compute_node_embedding.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(graph, node_embedding_method_name, fit_kwargs, verbose, support_mirrored_strategy, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Fitting the node embedding model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     history = model.fit(\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kg-env/lib/python3.8/site-packages/embiggen-0.9.3-py3.8.egg/embiggen/embedders/node2vec.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, early_stopping_monitor, early_stopping_min_delta, early_stopping_patience, early_stopping_mode, reduce_lr_monitor, reduce_lr_min_delta, reduce_lr_patience, reduce_lr_mode, reduce_lr_factor, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mDataframe\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m--> 212\u001b[0;31m         return self._model.fit(\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kg-env/lib/python3.8/site-packages/embiggen-0.9.3-py3.8.egg/embiggen/embedders/embedder.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, early_stopping_min_delta, early_stopping_patience, reduce_lr_min_delta, reduce_lr_patience, epochs, early_stopping_monitor, early_stopping_mode, reduce_lr_monitor, reduce_lr_mode, reduce_lr_factor, verbose, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_verbose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"callbacks\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         return pd.DataFrame(self._model.fit(\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kg-env/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kg-env/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: in user code:\n\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/six.py\", line 719, in reraise\n        raise value\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/engine/training.py\", line 816, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 532, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 639, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 830, in _create_all_weights\n        self._create_slots(var_list)\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/nadam.py\", line 98, in _create_slots\n        self.add_slot(var, 'v')\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 916, in add_slot\n        weight = tf.Variable(\n    File \"/home/harry/kg-env/lib/python3.8/site-packages/keras/initializers/initializers_v2.py\", line 144, in __call__\n        return tf.zeros(shape, dtype)\n\n    ResourceExhaustedError: OOM when allocating tensor with shape[944952,100] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n"
     ]
    }
   ],
   "source": [
    "# Now do a CBOW embedding and visualize the training.\n",
    "\n",
    "node_embedding_method_name = \"CBOW\"\n",
    "\n",
    "first_order_rw_node_embedding, training_history = compute_node_embedding(\n",
    "    g,\n",
    "    node_embedding_method_name=node_embedding_method_name,\n",
    ")\n",
    "\n",
    "plot_history(\n",
    "    training_history,\n",
    "    title=\"First-order random walk based {} model applied to graph {}\".format(\n",
    "        node_embedding_method_name,\n",
    "        g.get_name()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c934e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
